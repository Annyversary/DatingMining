{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c4b184f47e084a869d89315f623113a6",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# **Data Cleansing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ad3dcaccb024ea192c3c74084b0501b",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Load the Data  \n",
    "Between 2002 and 2004, Columbia University conducted a speed-dating experiment, tracking data from **21 speed-dating sessions** involving mostly young adults meeting people of the opposite sex. The dataset and its accompanying data key can be found here: http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/\n",
    "\n",
    "A total of 551 individuals participated, consisting of 277 men and 274 women. The dataset contains **8,378 individual observations**, with each row representing a speed date between two individuals and including **194 features**. Notably, the dataset only includes heterosexual pairings, which makes it somewhat outdated from a modern perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cell_id": "d16ddb9c10064ae8b218c1f534485099",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "2dff60c8-2c90-44fb-ab5c-e9d743c1fc04",
    "execution_millis": 3013,
    "execution_start": 1733669857361,
    "scrolled": true,
    "source_hash": "c3324e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iid   id  gender  idg  condtn  wave  round  position  positin1  order  ...  \\\n",
      "0    1  1.0       0    1       1     1     10         7       NaN      4  ...   \n",
      "1    1  1.0       0    1       1     1     10         7       NaN      3  ...   \n",
      "2    1  1.0       0    1       1     1     10         7       NaN     10  ...   \n",
      "3    1  1.0       0    1       1     1     10         7       NaN      5  ...   \n",
      "4    1  1.0       0    1       1     1     10         7       NaN      7  ...   \n",
      "\n",
      "   attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  intel5_3  \\\n",
      "0      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
      "1      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
      "2      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
      "3      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
      "4      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
      "\n",
      "   fun5_3  amb5_3  \n",
      "0     NaN     NaN  \n",
      "1     NaN     NaN  \n",
      "2     NaN     NaN  \n",
      "3     NaN     NaN  \n",
      "4     NaN     NaN  \n",
      "\n",
      "[5 rows x 195 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/original/speed_dating_data.csv', encoding='latin1')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes numerical ratings that participants assigned to six attributes they seek in their speed dating partners: Attractiveness (`attrX_Y`), Sincerity (`sincX_Y`), Intelligence (`intelX_Y`), Fun (`funX_Y`), Ambition (`ambX_Y`), and Shared Interests (`sharX_Y`). These attributes were rated at four different points in time:\n",
    "1. Before the event, as part of a survey completed by students interested in participating to register for the event.\n",
    "2. Halfway through the event, after meeting all potential dates, recorded on their scorecards.\n",
    "3. The day after the event, when participants filled out a follow-up survey.\n",
    "4. Three to four weeks later, when participants filled out a final survey after being sent their matches.\n",
    "\n",
    "These features are queried in five different ways:\n",
    "1. ”What do you look for in the opposite sex?”\n",
    "2. ”What do you think most of your fellow men/women look for in the opposite sex?”\n",
    "3. ”What do you think the opposite sex looks for in a date?”\n",
    "4. ”How do you think you measure up?”\n",
    "5. ”Finally, how do you think others perceive you?”\n",
    "\n",
    "The timing of the question is recorded in the attribute at the position of \"Y,\" while the way the question is framed is indicated at the position of \"X.\" For example, a value in the feature `attr1_1` represents **what an individual looks for in the opposite sex** at the time **before the first speed date**.\n",
    "\n",
    "The dataset also includes a broad array of personal characteristics, ranging from demographics and self-assessments to perceptions of lifestyle preferences, personal interests, income, study field, and career."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d2a9d7a70c1647e38dc6b51f7c3231a1",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Check for Duplicates \n",
    "\n",
    "First, we check whether the dataset contains any duplicates that need to be removed. However, no duplicates were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cell_id": "5226fd06e905447db56f0472eca1a31d",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "a97529ed-63ce-4b33-ad92-569690efe8da",
    "execution_millis": 126,
    "execution_start": 1733669860426,
    "source_hash": "b958f09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the DataFrame\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d422c1024cd34ca89bd3d2257da1aad6",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Extract Target Feature\n",
    "\n",
    "Our target feature of interest is `match`, which originally indicates whether both participants agreed to meet again. Later, our task will be to predict this variable. The dataset is fully supervised and has no missing entries in the target variable. As we can see, the dataset is imbalanced, with approximately **83% of instances having `match = 0`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": "0dec93604187430b8f499334cdc96002",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 1,
    "execution_start": 1733669860610,
    "source_hash": "cfc7cf0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match\n",
      "0    6998\n",
      "1    1380\n",
      "Name: count, dtype: int64\n",
      "Target (match):\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: match, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract 'match' as target variables\n",
    "target_match = df['match']\n",
    "\n",
    "# Check how often a match/no match occurs\n",
    "count_match_values = target_match.value_counts()\n",
    "print(count_match_values)\n",
    "\n",
    "# Remove 'match' column from the DataFrame\n",
    "features = df.drop(columns=['match'])\n",
    "\n",
    "# Check if the operation was successful\n",
    "print(\"Target (match):\")\n",
    "print(target_match.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "157f467271da44fc962ab3bd9e7c7821",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Dropping Features \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.1 because we are not interested\n",
    "\n",
    "The authors of the dataset sent an additional questionnaire to the participants 3-4 weeks after they had received their matches. Since we are only interested in the speed dates themselves, specifically whether a match occurred on the evening of the speed dating event, and not in whether or how a subsequent date took place, we will **drop the related features** (columns 118 to 194).\n",
    "\n",
    "The feature `match` equals 1 when both `dec` and `dec_o` are 1. Therefore, we need to drop both features. Otherwise, predicting matches later would be trivial.\n",
    "\n",
    "The categorical features `field` and `career` have been numerically encoded into broader categories as `field_cd` and `career_c`, respectively. We believe that these encoded features are sufficient for our prediction task, so we will **drop** `field` and `career`. \n",
    "\n",
    "Furthermore, we are **not interested** in features like `id` (_subject number within wave_), `idg` (subject number within gender, group ID/gender) or `partner` (_partner’s ID number the night of the event_). We want to understand what people look for in the opposite sex, and we don’t want our model to make decisions based on whether individuals match due to their IDs.\n",
    "\n",
    "Additionally, we are **not interested** in identifying the specific `round` in which a match occurred, nor in the `position1` where participants started or the `position` at which they matched, or the `order` (_the number of the date that night when the partner was met_). The feature `condtn` (_1 = limited choice, 2 = extensive choice_) also raises questions regarding its interpretation. Therefore, these features will be dropped as well.\n",
    "\n",
    "We also dropped `exhappy` (_\"Overall, on a scale of 1-10, how happy do you expect to be with the people you meet during the speed-dating event?\"_) because it is specific to the speed-dating context and does not represent an authentic question in real-life dating scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cell_id": "174eda21b8334da68f5aa621d284c58e",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 1,
    "execution_start": 1733669860702,
    "source_hash": "d95c316b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features before dropping is: 194\n",
      "The number of features after dropping is: 105\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features\n",
    "print(f\"The number of features before dropping is: {features.shape[1]}\")\n",
    "\n",
    "# Drop columns 118 to 194\n",
    "features = features.drop(features.columns[118:195], axis=1)\n",
    "\n",
    "# List of additional features to drop\n",
    "features_to_drop = ['dec', 'dec_o', 'career', 'round', 'order', 'exphappy', 'position', 'positin1', 'condtn', 'field', 'id', 'idg', 'partner']\n",
    "\n",
    "# Drop the specified features\n",
    "features = features.drop(columns=features_to_drop)\n",
    "\n",
    "# Print the number of remaining features\n",
    "print(f\"The number of features after dropping is: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "49b85da52f1546e28761ce840e41d0c8",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.2 because of > 75% missing values\n",
    "\n",
    "We initially chose a rather conservative **threshold of 75%** missing values for determining whether a feature should be dropped. First, we will examine which features have fewer than 2,100 documented instances (approximately 25%). This only applies to the feature `expnum`, which has 1,800 instances. The next feature with the most missing values is `mn_sat`, which has 3,133 instances (approximately 63% missing values). \n",
    "\n",
    "At this stage, we want to avoid being too strict and have therefore decided on a high threshold. Later, during feature selection, we will evaluate whether features with a high proportion of missing values provide less predictive power. For now, we will retain them.\n",
    "\n",
    "`expnum` represents the question: \"Out of the 20 people you will meet, how many do you expect will be interested in dating you?\" While it is an interesting question, we can afford to drop this feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cell_id": "1bfbc10bce7544bdaa4560058b9add05",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 4,
    "execution_start": 1733669860754,
    "source_hash": "891fbf02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with fewer than 2100 filled instances:\n",
      "expnum    1800\n",
      "dtype: int64\n",
      "The number of remaining features in the DataFrame is: 104\n"
     ]
    }
   ],
   "source": [
    "# Get the count of non-null values for each feature\n",
    "non_null_counts = features.count()\n",
    "\n",
    "# Filter for features with less than 2100 non-null values\n",
    "features_with_fewer_than_2100 = non_null_counts[non_null_counts < 2100]\n",
    "\n",
    "print(\"Features with fewer than 2100 filled instances:\")\n",
    "print(features_with_fewer_than_2100)\n",
    "\n",
    "# Drop the 'expnum' column\n",
    "features = features.drop('expnum', axis=1)\n",
    "\n",
    "# Print the number of remaining features\n",
    "print(f\"The number of remaining features in the DataFrame is: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d947117bd7f5450b93ab0899b1935143",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Adjust Categorical and Numerical Feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.1 converting mistakenly categorical features into numerical\n",
    "\n",
    "We first examine the categorical features in our dataset and notice that `income`, `tuition` (_\"Tuition listed for each response to undergrad in Barron’s 25th Edition college profile book\"_), and `mn_sat` (_\"Median SAT score for the undergraduate institution attended, taken from Barron’s 25th Edition college profile book. Proxy for intelligence\"_) are **listed as categorical**. However, we believe this is a misinterpretation because the numbers contain commas (e.g. \"65,929.00\"), which caused them to be treated as strings.\n",
    "\n",
    "We propose **converting these into numerical features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cell_id": "b7e8fb18af9d4731b99b5ae424314cb9",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 1,
    "execution_start": 1733669861150,
    "source_hash": "7c8c3071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories for each categorical feature:\n",
      "undergra    241\n",
      "mn_sat       68\n",
      "tuition     115\n",
      "from        269\n",
      "zipcode     409\n",
      "income      261\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical features (assuming 'object' dtype represents categorical features)\n",
    "categorical_features = features.select_dtypes(include=['object'])\n",
    "\n",
    "# For each categorical feature, count the number of unique categories\n",
    "unique_categories = categorical_features.nunique()\n",
    "\n",
    "# Display the number of unique categories for each categorical feature\n",
    "print(\"Number of unique categories for each categorical feature:\")\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_id": "ba309b1c0d174defaa2974cfd907af79",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 1,
    "execution_start": 1733669861222,
    "source_hash": "409ab6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      4279.000000\n",
      "mean      44887.606450\n",
      "std       17206.920962\n",
      "min        8607.000000\n",
      "25%       31516.000000\n",
      "50%       43185.000000\n",
      "75%       54303.000000\n",
      "max      109031.000000\n",
      "Name: income, dtype: float64\n",
      "count     3583.000000\n",
      "mean     21174.926040\n",
      "std       6748.661162\n",
      "min       2406.000000\n",
      "25%      15162.000000\n",
      "50%      25020.000000\n",
      "75%      26562.000000\n",
      "max      34300.000000\n",
      "Name: tuition, dtype: float64\n",
      "count    3133.000000\n",
      "mean     1299.655282\n",
      "std       119.798020\n",
      "min       914.000000\n",
      "25%      1214.000000\n",
      "50%      1310.000000\n",
      "75%      1400.000000\n",
      "max      1490.000000\n",
      "Name: mn_sat, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Remove the commas from the 'income', 'tuition' and 'mn_sat' columns\n",
    "features['income'] = features['income'].str.replace(',', '')\n",
    "features['tuition'] = features['tuition'].str.replace(',', '')\n",
    "features['mn_sat'] = features['mn_sat'].str.replace(',', '')\n",
    "\n",
    "# convert 'income', 'tuition', 'mn_sat' to numeric\n",
    "features['income'] = pd.to_numeric(features['income'], errors='coerce')\n",
    "features['tuition'] = pd.to_numeric(features['tuition'], errors='coerce')\n",
    "features['mn_sat'] = pd.to_numeric(features['mn_sat'], errors='coerce')\n",
    "\n",
    "# check the result\n",
    "print(features['income'].describe())\n",
    "print(features['tuition'].describe())\n",
    "print(features['mn_sat'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "21ff4b9ee0eb4a83879ff4f1ee9fc200",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2 converting numerical encoded features into categorical\n",
    "\n",
    "We discovered that certain **categorical variables in the dataset had already been numerically encoded**. However, this encoding implied an order that is not meaningful for some of the categorical data. To address this, we decoded the following columns back to their original categorical form: `field_cd`, `race`, `race_o`, `goal`, and `career_c`.\n",
    "\n",
    "For the remaining numerical encodings, we believe they are appropriate, as they either represent a binary relationship or reflect an inherent ordering among the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cell_id": "2bca824913f54fb5a9716caf701ef1d7",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 1,
    "execution_start": 1733669861286,
    "source_hash": "9683a2bc"
   },
   "outputs": [],
   "source": [
    "# Step 1: Define the decoding mappings for each of the relevant columns\n",
    "field_cd_mapping = {\n",
    "    1: 'Law', 2: 'Math', 3: 'Social Science', 4: 'Medical Science', 5: 'Engineering', \n",
    "    6: 'English', 7: 'History', 8: 'Business', 9: 'Education', 10: 'Biological Sciences', \n",
    "    11: 'Social Work', 12: 'Undecided', 13: 'Political Science', 14: 'Film', \n",
    "    15: 'Fine Arts', 16: 'Languages', 17: 'Architecture', 18: 'Other'\n",
    "}\n",
    "\n",
    "race_mapping = {\n",
    "    1: 'Black/African American', 2: 'European/Caucasian-American', \n",
    "    3: 'Latino/Hispanic American', 4: 'Asian/Pacific Islander/Asian-American', \n",
    "    5: 'Native American', 6: 'Other'\n",
    "}\n",
    "\n",
    "goal_mapping = {\n",
    "    1: 'Fun Night Out', 2: 'Meet New People', 3: 'Get a Date', \n",
    "    4: 'Serious Relationship', 5: 'To Say I Did It', 6: 'Other'\n",
    "}\n",
    "\n",
    "career_c_mapping = {\n",
    "    1: 'Lawyer', 2: 'Academic/Research', 3: 'Psychologist', 4: 'Doctor/Medicine', \n",
    "    5: 'Engineer', 6: 'Creative Arts/Entertainment', 7: 'Banking/Finance', \n",
    "    8: 'Real Estate', 9: 'International Affairs', 10: 'Undecided', 11: 'Social Work', \n",
    "    12: 'Speech Pathology', 13: 'Politics', 14: 'Pro Sports/Athletics', 15: 'Other', \n",
    "    16: 'Journalism', 17: 'Architecture'\n",
    "}\n",
    "\n",
    "# Step 2: Replace the numerical values in the corresponding columns with their decoded string equivalents\n",
    "features['field_cd'] = features['field_cd'].map(field_cd_mapping).astype('object')\n",
    "features['race'] = features['race'].map(race_mapping).astype('object')\n",
    "features['race_o'] = features['race_o'].map(race_mapping).astype('object')\n",
    "features['goal'] = features['goal'].map(goal_mapping).astype('object')\n",
    "features['career_c'] = features['career_c'].map(career_c_mapping).astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories for each categorical feature:\n",
      "race_o        5\n",
      "field_cd     18\n",
      "undergra    241\n",
      "race          5\n",
      "from        269\n",
      "zipcode     409\n",
      "goal          6\n",
      "career_c     17\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical features (assuming 'object' dtype represents categorical features)\n",
    "categorical_features = features.select_dtypes(include=['object'])\n",
    "\n",
    "# For each categorical feature, count the number of unique categories\n",
    "unique_categories = categorical_features.nunique()\n",
    "\n",
    "# Display the number of unique categories for each categorical feature\n",
    "print(\"Number of unique categories for each categorical feature:\")\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1eb2a72738414ebd9f27b76931d9dd55",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## 5.3 map categorical features\n",
    "\n",
    "For the features `from` and `undergra`, this is not the case. These features were collected through open-ended questions without predefined answer choices. As a result, some entries contain joke responses or variations of the same answer expressed in slightly different ways. This has led to these variations being treated as distinct values within these categorical features. Our goal is to **identify and normalize these inconsistencies to avoid creating unnecessary features during one-hot encoding** later on.\n",
    "\n",
    "At this stage, we manually reviewed the dataset and performed the mapping ourselves. We do not rule out the possibility that a library exists for this task. However, given the size of the dataset, the manual effort was manageable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts in 'from':\n",
      "from\n",
      "New York          522\n",
      "New Jersey        365\n",
      "California        301\n",
      "China             139\n",
      "Italy             132\n",
      "                 ... \n",
      "Greenwich, CT       5\n",
      "Europe              5\n",
      "sofia, bg           5\n",
      "Pougkeepsie NY      5\n",
      "china               5\n",
      "Name: count, Length: 269, dtype: int64\n",
      "\n",
      "Value counts in 'undergra':\n",
      "undergra\n",
      "UC Berkeley                107\n",
      "Harvard                    104\n",
      "Columbia                    95\n",
      "Yale                        86\n",
      "NYU                         78\n",
      "                          ... \n",
      "medicine                     6\n",
      "Rice University              6\n",
      "University of Rochester      6\n",
      "China                        6\n",
      "University of Florida        6\n",
      "Name: count, Length: 241, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Frequency counts for 'from'\n",
    "print(\"\\nValue counts in 'from':\")\n",
    "print(features['from'].value_counts())\n",
    "\n",
    "# Frequency counts for 'undergra'\n",
    "print(\"\\nValue counts in 'undergra':\")\n",
    "print(features['undergra'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **mapped** the `from` entries according to the following rules:\n",
    "\n",
    "1. All entries that semantically express the same location were grouped under a single label (e.g., `New York`).\n",
    "\n",
    "2. Entries located overseas were mapped to their corresponding countries (e.g., `'Paris'` → `'France'`).\n",
    "\n",
    "3. Entries that could not be associated with a city or country were categorized as `'Other'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_mapping = {\n",
    "    # 1. Map all entries that semantically express the same location to a single label\n",
    "    'New York City': 'New York City',  \n",
    "    'NYC': 'New York City',\n",
    "    'New York, NY': 'New York City',\n",
    "    'I am from NYC': 'New York City',\n",
    "    'NY': 'New York City',\n",
    "    'NYC-6 yrs. Grew up in Nebraska': 'New York City',\n",
    "    'nyc': 'New York City',\n",
    "    'new york': 'New York City',\n",
    "    'new york city': 'New York City',\n",
    "    'Upstate New York': 'New York City',\n",
    "      \n",
    "    'NYC (Staten Island)': 'Staten Island',     \n",
    "    'Long Island': 'Long Island, NY',\n",
    "    \n",
    "    'brooklyn ny': 'Brooklyn NY',\n",
    "    'Brooklyn': 'Brooklyn NY',\n",
    "    'brooklyn, ny': 'Brooklyn NY',\n",
    "\n",
    "    'Westchester, new York': 'Westchester, NY',\n",
    "    'Westchester County, N.Y.': 'Westchester, NY',\n",
    "    \n",
    "    'Cherry Hill, NJ': 'New Jersey',\n",
    "    'NJ': 'New Jersey',\n",
    "    'new jersey': 'New Jersey',\n",
    "    'South Orange, New Jersey': 'New Jersey',\n",
    "    'New York Area/ New Jersey': 'New Jersey',\n",
    "    \n",
    "    'Boston, MA': 'Boston',\n",
    "    'boston, ma': 'Boston',\n",
    "    'Boston, Ma': 'Boston',\n",
    "\n",
    "    'Palo Alto, California': 'California',\n",
    "    'Palo Alto, CA': 'California',\n",
    "    'Palm Springs, California': 'California',\n",
    "    'California (West Coast)': 'California',\n",
    "    'Santa Barbara, California': 'California',\n",
    "    'california': 'California',\n",
    "    'CALIFORNIA': 'California',\n",
    "\n",
    "    'lOS aNGELES': 'Los Angeles',\n",
    "    'Los Angeles, CA': 'Los Angeles',\n",
    "    'los angeles': 'Los Angeles',\n",
    "\n",
    "    'San Francisco, CA': 'San Francisco',\n",
    "    'San Francisco Bay Area': 'San Francisco',\n",
    "    'SF Bay Area, CA': 'San Francisco',\n",
    "    '94115': 'San Francisco', # Zipcode\n",
    "    'San Francisco(home)/Los Angeles(undergrad)': 'San Francisco',\n",
    "\n",
    "    'Washington, D.C.': 'Washington DC',  \n",
    "    'WASHINGTON, D.C.': 'Washington DC',\n",
    "    'Washington DC Metro Region': 'Washington DC',\n",
    "    'DC': 'Washington DC',\n",
    "    'Wash DC (4 yrs)': 'Washington DC',\n",
    "\n",
    "    'Philadelphia, PA': 'Philadelphia',\n",
    "    'Born in Montana, raised in South Jersey (nr. Philadelphia)': 'Philadelphia',\n",
    "\n",
    "    'Atlanta': 'Atlanta, GA',\n",
    "    'atlanta, ga': 'Atlanta, GA',\n",
    "    \n",
    "    'colorado': 'Colorado',\n",
    "    'Boulder, Colorado': 'Colorado',\n",
    "\n",
    "    'Toronto, Canada': 'Toronto',\n",
    "   \n",
    "    'Detroit, Michigan, USA': 'Detroit',\n",
    "    'Detroit suburbs': 'Detroit',\n",
    "\n",
    "    'Tuscaloosa, Alabama': 'Alabama',\n",
    "    'alabama': 'Alabama',\n",
    "\n",
    "    'San Diego, CA': 'San Diego',\n",
    "    'Dallas, Texas': 'Texas',\n",
    "    'Cincinnati, OH': 'Cincinnati, Ohio',\n",
    "    'Ann Arbor': 'Ann Arbor, MI',\n",
    "    'Minneapolis, MN': 'Minneapolis',\n",
    "    'Pittsburgh, PA': 'Pittsburgh',\n",
    "    'Berkeley, CA': 'Berkeley',\n",
    "    'ottawa, canada': 'Ottawa, Canada',\n",
    "    'MD': 'Maryland',\n",
    "    'TN': 'Tennessee',\n",
    "    'PA': 'Pennsylvania',  \n",
    "    \n",
    "    # 2. Map cities located outside of the US to their corresponding countries\n",
    "    'Genova, Italy': 'Italy',\n",
    "    'Milan, Italy': 'Italy',\n",
    "    'Milano, Italy': 'Italy',\n",
    "    'Milan - Italy': 'Italy',\n",
    "\n",
    "    'china': 'China',\n",
    "    'BEIJING, CHINA': 'China',\n",
    "    'Shanghai, China': 'China',\n",
    "    'P. R. China': 'China',\n",
    "\n",
    "    'New Delhi, India': 'India',\n",
    "    'Bombay, India': 'India',  \n",
    "    'india': 'India',\n",
    "\n",
    "    'Taipei, Taiwan': 'Taiwan',\n",
    "    'taiwan': 'Taiwan',\n",
    "   \n",
    "    'france': 'France',\n",
    "    'Paris': 'France',\n",
    "    \n",
    "    'philippines': 'Philippines',\n",
    "    'Manila, Philippines': 'Philippines',\n",
    "\n",
    "    'SIngapore': 'Singapore',\n",
    "    'Asia, Singapore': 'Singapore',\n",
    "\n",
    "    'Colombia, South America': 'Colombia',\n",
    "    'Bogota, Colombia': 'Colombia',\n",
    "\n",
    "    'Tokyo, Japan': 'Japan',\n",
    "    'japan': 'Japan',\n",
    "  \n",
    "    'SOUTH KOREA': 'South Korea',\n",
    "    'KOREA': 'South Korea',\n",
    "    'Korea': 'South Korea',\n",
    "\n",
    "    'Warsaw, Poland': 'Poland',\n",
    "    'poland': 'Poland',\n",
    "    'spain': 'Spain',\n",
    "    'HKG': 'Hong Kong',\n",
    "    'Born in Iran': 'Iran',\n",
    "    'uruguay': 'Uruguay',\n",
    "    'sofia, bg': 'Bulgaria',    \n",
    "    'London, UK': 'London',\n",
    "  \n",
    "    # 3. other\n",
    "    'working': 'Other',\n",
    "    'International Students': 'Other',\n",
    "    'Brandeis University': 'Other',\n",
    "    'State College, PA': 'Other',\n",
    "    'way too little space here. world citizen.': 'Other',\n",
    "    'Bowdoin College': 'Other',\n",
    "    'USA/American': 'Other',\n",
    "    'Europe': 'Other',\n",
    "}\n",
    "\n",
    "features['from'] = features['from'].replace(from_mapping)\n",
    "#features['from'].to_csv('from.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **mapped** the `undergra` entries according to the following rules:\n",
    "\n",
    "1. Entries referring to the same university, despite variations in wording or formatting, were grouped under a single standardized label.\n",
    "\n",
    "2. Entries that could not be clearly associated with a specific university or field of study were categorized as `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "undergra_mapping = {\n",
    "\n",
    "    'U.C. Berkeley': 'UC Berkeley',\n",
    "    'Harvard University': 'Harvard',\n",
    "    'harvard': 'Harvard',\n",
    "    'ColumbiaU': 'Columbia',\n",
    "    'Columbia University': 'Columbia',\n",
    "    'Yale University': 'Yale',\n",
    "    'NYU': 'New York University',\n",
    "    'nyu': 'New York University',\n",
    "    'Brown University': 'Brown',\n",
    "    'ucla': 'UCLA',\n",
    "    'Cornell University': 'Cornell',\n",
    "    'Tufts': 'Tufts University',\n",
    "    'tufts': 'Tufts University',\n",
    "    'Rutgers University - New Brunswick': 'Rutgers',\n",
    "    'Rutgers University': 'Rutgers',\n",
    "    'university of pennsylvania': 'University of Pennsylvania',\n",
    "    'Univ of Pennsylvania': 'University of Pennsylvania',\n",
    "    'UPenn': 'University of Pennsylvania',\n",
    "    'U of  Michigan': 'University of Michigan',\n",
    "    'University of Michigan-Ann Arbor': 'University of Michigan',\n",
    "    'Univeristy of Michigan': 'University of Michigan',\n",
    "    'U of Vermont': 'University of Vermont',\n",
    "    'Univ. of Connecticut': 'University of Connecticut',\n",
    "    'Washington U. in St. Louis': 'Washington University in St. Louis',\n",
    "    'washington university in st louis': 'Washington University in St. Louis',\n",
    "    'Delhi University': 'University of Delhi',\n",
    "    'Georgetown': 'Georgetown University',\n",
    "    'UC, IRVINE!!!!!!!!!': 'University of California, Irvine',\n",
    "    'UC Irvine': 'University of California, Irvine',\n",
    "    'Princeton University': 'Princeton',\n",
    "    'Princeton U..': 'Princeton',\n",
    "    'Stanford': 'Stanford University',\n",
    "    'Univeristy of California, Davis': 'University of California, Davis',\n",
    "    'COOPER UNION': 'Cooper Union',\n",
    "    'Cooper Union, Bard college, and SUNY Purchase': 'Cooper Union',\n",
    "    'Loyola College in Maryland': 'Loyola College',\n",
    "    'Oxford University': 'Oxford',\n",
    "    'u of southern california, economics': 'University of Southern California',\n",
    "    'UW Madison': 'University of Wisconsin-Madison',\n",
    "    'Rice': 'Rice University',\n",
    "    'warsaw university': 'Warsaw University',\n",
    "    'umass': 'University of Massachusetts-Amherst',\n",
    "    'University of Illinois/Champaign': 'Illinois',\n",
    "    'university of the philippines': 'University of the Philippines',\n",
    "    'University of California at Santa Cruz': 'University of California, Santa Cruz',\n",
    "    'UC Santa Cruz': 'University of California, Santa Cruz',\n",
    "    'UM': 'University of Michigan',\n",
    "    'ecole polytechnique': 'Ecole Polytechnique',\n",
    "    'Ecole Polytechnique (France)': 'Ecole Polytechnique',\n",
    "    'Bombay, India': 'University of Bombay',\n",
    "    'GW': 'George Washington University',\n",
    "    'university of wisconsin/la crosse': 'University of Wisconsin',\n",
    "    'oberlin': 'Oberlin College',\n",
    "    'Fudan': 'Fudan University',\n",
    "\n",
    "    'Columbia College': 'Columbia',\n",
    "    'Columbia College, CU': 'Columbia',\n",
    "    'Harvard College': 'Harvard',\n",
    "    'Rutgers College': 'Rutgers',\n",
    "    'Connecticut College': 'University of Connecticut',\n",
    "    'Holy Cross College': 'Holy Cross',\n",
    "\n",
    "    'school overseas (need a name ?)': 'Other',\n",
    "    'The American University': 'Other',\n",
    "}\n",
    "\n",
    "features['undergra'] = features['undergra'].replace(undergra_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to map the `zipcodes` at this stage, as the number of features after one-hot encoding seemed excessive.\n",
    "\n",
    "We map US zip codes to their corresponding state abbreviations. If the zip code is missing or not found in the database, it returns `Unknown`. Before applying this transformation, we clean the 'zipcode' column by removing commas and extra whitespace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of states for the zip codes:\n",
      " zipcode\n",
      "Unknown    3312\n",
      "NY         1753\n",
      "CA          820\n",
      "PA          354\n",
      "TX          250\n",
      "MD          234\n",
      "MI          179\n",
      "FL          141\n",
      "CO          132\n",
      "OH          112\n",
      "VA          105\n",
      "IL          104\n",
      "MN           94\n",
      "NC           78\n",
      "GA           76\n",
      "KS           70\n",
      "IN           59\n",
      "SC           51\n",
      "WI           49\n",
      "HI           46\n",
      "TN           40\n",
      "WA           39\n",
      "DC           39\n",
      "IA           37\n",
      "AL           35\n",
      "AZ           32\n",
      "MO           32\n",
      "NM           28\n",
      "NE           24\n",
      "UT           22\n",
      "NV           21\n",
      "LA           10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pyzipcode import ZipCodeDatabase\n",
    "\n",
    "# Initialize the ZipCodeDatabase\n",
    "zcdb = ZipCodeDatabase()\n",
    "\n",
    "# Function to map zip code to state using pyzipcode\n",
    "def get_state_from_zip(zip_code):\n",
    "    try:\n",
    "        # Ensure the zip code is valid and look it up\n",
    "        if pd.notna(zip_code) and zip_code != '':\n",
    "            zip_info = zcdb[zip_code]\n",
    "            return zip_info.state  # Returns the state abbreviation (e.g., CA, NY)\n",
    "    except:\n",
    "        # If ZIP code is not found in the database\n",
    "        return 'Unknown'\n",
    "    return 'Unknown'  # Default for missing or invalid ZIP codes\n",
    "\n",
    "# Replace commas and strip whitespace from 'zipcode' column\n",
    "features['zipcode'] = features['zipcode'].astype(str).str.replace(',', '').str.strip()\n",
    "\n",
    "# Apply pyzipcode transformation to map zip codes to states\n",
    "features['zipcode'] = features['zipcode'].apply(get_state_from_zip)\n",
    "\n",
    "# Show state distribution\n",
    "state_distribution = features['zipcode'].value_counts()\n",
    "print(\"Distribution of states for the zip codes:\\n\", state_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ccce8aeb0c44420fb2988781be82e180",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6. Perform Data Normalization\n",
    "\n",
    "By **data normalization**, we refer to the correction of values that contradict the original design of the survey, or values with different scales for the same feature. **Standardization** of the data, on the other hand, takes place at a later stage of preprocessing.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6.1 Normalize Features through different Waves\n",
    "\n",
    "`Wave` refers to the date when the event took place. In total, data was tracked across 21 speed dating sessions, which resulted in **21 distinct waves**. The authors of the dataset made several changes or adjustments to their questionnaire during the course of data collection.\n",
    "\n",
    "**Waves 6-9**: _Please rate the importance of the following attributes on a scale of 1-10 (1=not at all important, 10=extremely important)._\n",
    "\n",
    "**Waves 1-5 & 10-21**: _You have 100 points to distribute among the following attributes -- give more points to those attributes that you think your fellow men/women find more important in a potential date and fewer points to those attributes that they find less important in a potential date. Total points must equal 100._\n",
    "\n",
    "We begin by splitting our features into **two distinct dataframes**: one dataframe containing instances from waves 6-9 and another with all instances from the remaining waves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cell_id": "46b2dbc4de2845cbab2dec81b1f04347",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 1,
    "execution_start": 1733669860806,
    "source_hash": "bc039737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with wave between 6 and 9: 1562\n",
      "Rows with wave outside 6 and 9: 6816\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'wave' is between 6 and 9 (inclusive)\n",
    "df_wave_6_9 = features[features['wave'].between(6, 9)]\n",
    "\n",
    "# Filter rows where 'wave' is NOT between 6 and 9\n",
    "df_other_waves = features[~features['wave'].between(6, 9)]\n",
    "\n",
    "# Check the number of rows in each DataFrame\n",
    "print(f\"Rows with wave between 6 and 9: {len(df_wave_6_9)}\")\n",
    "print(f\"Rows with wave outside 6 and 9: {len(df_other_waves)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "67f6af1933a34386a64292495a25a6fa",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We used the `describe()` command to display the description of all features and identified the following difference:\n",
    "\n",
    "Contrary to the documentation in the key sheet, the features `attr1_1`, `sinc1_1`, ... and `shar1_1` did not take integer values between 1 and 10. Instead, they displayed float values ranging from 2.27 to 27.78.\n",
    "\n",
    "We hypothesize that these features were preprocessed by the dataset authors in the following way:\n",
    "1. **Summing**: For each group of features, the authors likely summed the values within each row.\n",
    "2. **Proportion Calculation**: They then calculated the proportion of each feature relative to the row’s total sum.\n",
    "3. **Scaling**: Finally, they multiplied these proportions by 100 to convert them into point values.\n",
    "\n",
    "While the respective minimum and maximum values differ between features across dataframes, the features remain comparable in terms of the average values within each feature column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "cell_id": "cf56743060664cd7bca23f99b2b4da1f",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "ff135fda-029a-440e-935a-011df6188739",
    "execution_millis": 22,
    "execution_start": 1733669860862,
    "source_hash": "5902d58a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for df_wave_6_9:\n",
      "           attr1_1     sinc1_1     intel1_1       fun1_1       amb1_1  \\\n",
      "count  1557.000000  1557.00000  1557.000000  1557.000000  1557.000000   \n",
      "mean     16.158304    17.82194    18.990886    17.910328    14.733789   \n",
      "std       3.515382     2.75362     1.993004     2.440198     4.180549   \n",
      "min       6.670000     5.13000    14.710000    12.500000     2.330000   \n",
      "25%      14.290000    16.67000    17.390000    16.670000    13.040000   \n",
      "50%      16.000000    17.78000    18.870000    17.950000    15.690000   \n",
      "75%      18.000000    19.44000    20.000000    19.230000    17.780000   \n",
      "max      27.780000    23.81000    23.810000    27.780000    20.590000   \n",
      "\n",
      "           shar1_1  \n",
      "count  1557.000000  \n",
      "mean     14.386532  \n",
      "std       3.946962  \n",
      "min       2.270000  \n",
      "25%      12.500000  \n",
      "50%      14.890000  \n",
      "75%      17.070000  \n",
      "max      23.810000  \n",
      "\n",
      "Description for df_other_waves:\n",
      "           attr1_1      sinc1_1     intel1_1       fun1_1       amb1_1  \\\n",
      "count  6742.000000  6742.000000  6742.000000  6732.000000  6722.000000   \n",
      "mean     23.982565    17.298112    20.559999    17.352206     9.744158   \n",
      "std      13.442754     7.702178     7.433520     6.645362     6.121752   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%      15.000000    10.000000    18.000000    14.000000     5.000000   \n",
      "50%      20.000000    20.000000    20.000000    18.000000    10.000000   \n",
      "75%      30.000000    20.000000    25.000000    20.000000    15.000000   \n",
      "max     100.000000    60.000000    50.000000    50.000000    53.000000   \n",
      "\n",
      "           shar1_1  \n",
      "count  6700.000000  \n",
      "mean     11.254515  \n",
      "std       6.664553  \n",
      "min       0.000000  \n",
      "25%       5.000000  \n",
      "50%      10.000000  \n",
      "75%      15.000000  \n",
      "max      30.000000  \n"
     ]
    }
   ],
   "source": [
    "# Describe columns 'attr1_1', 'sinc1_1', ... and 'shar1_1'for df_wave_6_9\n",
    "desc_wave_6_9 = df_wave_6_9.iloc[:, 55:61].describe()\n",
    "\n",
    "# Describe columns 'attr1_1', 'sinc1_1', ... and'shar1_1 for df_other_waves\n",
    "desc_other_waves = df_other_waves.iloc[:, 55:61].describe()\n",
    "\n",
    "# Display both descriptions, one after the other\n",
    "print(\"Description for df_wave_6_9:\")\n",
    "print(desc_wave_6_9)\n",
    "\n",
    "print(\"\\nDescription for df_other_waves:\")\n",
    "print(desc_other_waves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b2d0180b235d4372a44bb06297e48769",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "However, the authors **did not consistently preprocess** all of their features using this approach. We noticed that the features `attr4_1`, `sinc4_1`, ... and `shar4_1` were documented in their scaled version, as indicated by their minimum and maximum values ranging from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for df_wave_6_9:\n",
      "           attr4_1      sinc4_1     intel4_1       fun4_1       amb4_1  \\\n",
      "count  1557.000000  1557.000000  1557.000000  1557.000000  1557.000000   \n",
      "mean      8.638407     7.030829     6.872832     8.077714     6.414258   \n",
      "std       1.141478     1.730291     1.789146     1.384266     2.281281   \n",
      "min       5.000000     3.000000     2.000000     4.000000     1.000000   \n",
      "25%       8.000000     6.000000     6.000000     7.000000     5.000000   \n",
      "50%       9.000000     7.000000     7.000000     8.000000     7.000000   \n",
      "75%      10.000000     8.000000     8.000000     9.000000     8.000000   \n",
      "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
      "\n",
      "           shar4_1  \n",
      "count  1557.000000  \n",
      "mean      6.886962  \n",
      "std       1.918672  \n",
      "min       1.000000  \n",
      "25%       6.000000  \n",
      "50%       7.000000  \n",
      "75%       8.000000  \n",
      "max      10.000000  \n",
      "\n",
      "Description for df_other_waves:\n",
      "           attr4_1      sinc4_1     intel4_1       fun4_1       amb4_1  \\\n",
      "count  4932.000000  4932.000000  4932.000000  4932.000000  4932.000000   \n",
      "mean     31.999797    12.347121    14.455799    17.931062    10.842660   \n",
      "std      14.767137     7.114781     6.675540     6.837787     7.621974   \n",
      "min       5.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%      20.000000    10.000000    10.000000    15.000000     5.000000   \n",
      "50%      30.000000    10.000000    15.000000    20.000000    10.000000   \n",
      "75%      40.000000    17.000000    20.000000    20.000000    15.000000   \n",
      "max      95.000000    35.000000    35.000000    45.000000    50.000000   \n",
      "\n",
      "           shar4_1  \n",
      "count  4910.000000  \n",
      "mean     12.323829  \n",
      "std       6.331559  \n",
      "min       0.000000  \n",
      "25%      10.000000  \n",
      "50%      10.000000  \n",
      "75%      15.000000  \n",
      "max      40.000000  \n"
     ]
    }
   ],
   "source": [
    "# Describe columns 'attr4_1', 'sinc4_1', ... and 'shar4_1' for df_wave_6_9\n",
    "desc_wave_6_9 = df_wave_6_9.iloc[:, 61:67].describe()\n",
    "\n",
    "# Describe columns 'attr4_1', 'sinc4_1', ... and 'shar4_1'for df_other_waves\n",
    "desc_other_waves = df_other_waves.iloc[:, 61:67].describe()\n",
    "\n",
    "# Display both descriptions, one after the other\n",
    "print(\"Description for df_wave_6_9:\")\n",
    "print(desc_wave_6_9)\n",
    "\n",
    "print(\"\\nDescription for df_other_waves:\")\n",
    "print(desc_other_waves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6faa52491b5c436b830d93d1ff4f7338",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We followed the assumption and **preprocessed these features accordingly**. We standardized the affected features as follows: For each group of features, we started by summing the values within each row. After calculating the total for each row, we determined the proportion of each feature relative to this sum. Finally, we multiplied the result by 100 to convert the proportions into point values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- First Feature Set (attr4_1, sinc4_1, etc.) ---\n",
      "\n",
      "        attr4_1    sinc4_1   intel4_1     fun4_1     amb4_1    shar4_1\n",
      "1846  23.255814  16.279070  16.279070  16.279070  11.627907  16.279070\n",
      "1847  23.255814  16.279070  16.279070  16.279070  11.627907  16.279070\n",
      "1848  23.255814  16.279070  16.279070  16.279070  11.627907  16.279070\n",
      "1849  23.255814  16.279070  16.279070  16.279070  11.627907  16.279070\n",
      "1850  23.255814  16.279070  16.279070  16.279070  11.627907  16.279070\n",
      "1851  17.073171  17.073171  17.073171  17.073171  14.634146  17.073171\n",
      "1852  17.073171  17.073171  17.073171  17.073171  14.634146  17.073171\n",
      "1853  17.073171  17.073171  17.073171  17.073171  14.634146  17.073171\n",
      "1854  17.073171  17.073171  17.073171  17.073171  14.634146  17.073171\n",
      "1855  17.073171  17.073171  17.073171  17.073171  14.634146  17.073171\n"
     ]
    }
   ],
   "source": [
    "# Define a function to process feature sets and add section headers\n",
    "def adjust_features(df, features, title):\n",
    "    # Print section header\n",
    "    print(f\"\\n--- {title} ---\\n\")\n",
    "    \n",
    "    # Sum the values row-wise for the specified features\n",
    "    row_sums = df[features].sum(axis=1)\n",
    "    \n",
    "    # Use .loc[] to explicitly update the DataFrame without triggering SettingWithCopyWarning\n",
    "    df.loc[:, features] = df[features].div(row_sums, axis=0) * 100\n",
    "    \n",
    "    # Check the first 10 rows after adjustment\n",
    "    print(df[features].head(10))\n",
    "\n",
    "# List of feature sets and their titles\n",
    "features_to_adjust_sets = [\n",
    "    (['attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1'], 'First Feature Set (attr4_1, sinc4_1, etc.)'),\n",
    "]\n",
    "\n",
    "# Apply the function to each feature set with its title\n",
    "for features, title in features_to_adjust_sets:\n",
    "    adjust_features(df_wave_6_9, features, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "39ca6f2487a74936bae7e9a4ffbbbc72",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Furthermore, we noticed the following difference between waves 6-9 and the remaining waves: The features `attr5_1`, `sinc5_1`, ... and `amb5_1` were **not documented in waves 6-9**.\n",
    "\n",
    "We assume that these features were introduced relatively late in the questionnaire, likely during the final waves. At this point, we have decided to retain these features despite the large number of missing values and plan to examine them more closely during the **feature selection** process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for df_wave_6_9:\n",
      "       attr5_1  sinc5_1  intel5_1  fun5_1  amb5_1\n",
      "count      0.0      0.0       0.0     0.0     0.0\n",
      "mean       NaN      NaN       NaN     NaN     NaN\n",
      "std        NaN      NaN       NaN     NaN     NaN\n",
      "min        NaN      NaN       NaN     NaN     NaN\n",
      "25%        NaN      NaN       NaN     NaN     NaN\n",
      "50%        NaN      NaN       NaN     NaN     NaN\n",
      "75%        NaN      NaN       NaN     NaN     NaN\n",
      "max        NaN      NaN       NaN     NaN     NaN\n",
      "\n",
      "Description for df_other_waves:\n",
      "           attr5_1      sinc5_1     intel5_1       fun5_1       amb5_1\n",
      "count  4906.000000  4906.000000  4906.000000  4906.000000  4906.000000\n",
      "mean      6.941908     7.927232     8.284346     7.426213     7.617611\n",
      "std       1.498653     1.627054     1.283657     1.779129     1.773094\n",
      "min       2.000000     1.000000     3.000000     2.000000     1.000000\n",
      "25%       6.000000     7.000000     8.000000     6.000000     7.000000\n",
      "50%       7.000000     8.000000     8.000000     8.000000     8.000000\n",
      "75%       8.000000     9.000000     9.000000     9.000000     9.000000\n",
      "max      10.000000    10.000000    10.000000    10.000000    10.000000\n"
     ]
    }
   ],
   "source": [
    "# Describe columns 'attr5_1' 'sinc5_1', ... and 'amb5_1' for df_wave_6_9\n",
    "desc_wave_6_9 = df_wave_6_9.iloc[:, 78:83].describe()\n",
    "\n",
    "# Describe columns 'attr5_1' 'sinc5_1', ... and 'amb5_1' for df_other_waves\n",
    "desc_other_waves = df_other_waves.iloc[:, 78:83].describe()\n",
    "\n",
    "# Display both descriptions, one after the other\n",
    "print(\"Description for df_wave_6_9:\")\n",
    "print(desc_wave_6_9)\n",
    "\n",
    "print(\"\\nDescription for df_other_waves:\")\n",
    "print(desc_other_waves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "72883abb987547d9b8dbb9159c8d3ea2",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Finally, we **merged the two dataframes again**.\n",
    "\n",
    "Since we are **not interested in** `wave` as a feature, we don't want our model to consider it in its decision-making, as it provides no insight into what people look for in the opposite sex. Therefore, we are dropping it at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the combined DataFrame is: 8378\n",
      "The number of features (columns) in the combined DataFrame is: 103\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames row-wise\n",
    "features = pd.concat([df_wave_6_9, df_other_waves], axis=0)\n",
    "\n",
    "# Reset the index if needed (since after concatenation, the index might not be continuous)\n",
    "features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop the 'wave' feature\n",
    "features = features.drop(columns=['wave'])\n",
    "\n",
    "print(f\"The length of the combined DataFrame is: {len(features)}\")\n",
    "print(f\"The number of features (columns) in the combined DataFrame is: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6.2. Normalize Numerical Features collected using the Likert Scale\n",
    "\n",
    "We examine the features that should be rated on a scale from 1 to 10 according to the survey. Any value outside this range is most likely an exaggerated response from the participant. In our dataset, we observe that the maximum values for features such as `gaming` and `reading` exceed 10. Similarly, features like `hiking`, `clubbing`, and `gaming` fall below the expected scale, with some values being 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "cell_id": "34a2b73a8a964ce3b345ef3bc4378611",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "5efddbe5-ecd2-470d-96b7-2ac21ee357b3",
    "execution_millis": 1,
    "execution_start": 1733669861338,
    "source_hash": "32ccbad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            sports     tvsports     exercise       dining      museums  \\\n",
      "count  8299.000000  8299.000000  8299.000000  8299.000000  8299.000000   \n",
      "mean      6.425232     4.575491     6.245813     7.783829     6.985781   \n",
      "std       2.619024     2.801874     2.418858     1.754868     2.052232   \n",
      "min       1.000000     1.000000     1.000000     1.000000     0.000000   \n",
      "25%       4.000000     2.000000     5.000000     7.000000     6.000000   \n",
      "50%       7.000000     4.000000     6.000000     8.000000     7.000000   \n",
      "75%       9.000000     7.000000     8.000000     9.000000     9.000000   \n",
      "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
      "\n",
      "               art       hiking       gaming     clubbing      reading  \\\n",
      "count  8299.000000  8299.000000  8299.000000  8299.000000  8299.000000   \n",
      "mean      6.714544     5.737077     3.881191     5.745993     7.678515   \n",
      "std       2.263407     2.570207     2.620507     2.502218     2.006565   \n",
      "min       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
      "25%       5.000000     4.000000     2.000000     4.000000     7.000000   \n",
      "50%       7.000000     6.000000     3.000000     6.000000     8.000000   \n",
      "75%       8.000000     8.000000     6.000000     8.000000     9.000000   \n",
      "max      10.000000    10.000000    14.000000    10.000000    13.000000   \n",
      "\n",
      "                tv      theater       movies     concerts        music  \\\n",
      "count  8299.000000  8299.000000  8299.000000  8299.000000  8299.000000   \n",
      "mean      5.304133     6.776118     7.919629     6.825401     7.851066   \n",
      "std       2.529135     2.235152     1.700927     2.156283     1.791827   \n",
      "min       1.000000     0.000000     0.000000     0.000000     1.000000   \n",
      "25%       3.000000     5.000000     7.000000     5.000000     7.000000   \n",
      "50%       6.000000     7.000000     8.000000     7.000000     8.000000   \n",
      "75%       7.000000     9.000000     9.000000     8.000000     9.000000   \n",
      "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
      "\n",
      "          shopping         yoga  \n",
      "count  8299.000000  8299.000000  \n",
      "mean      5.631281     4.339197  \n",
      "std       2.608913     2.717612  \n",
      "min       1.000000     0.000000  \n",
      "25%       4.000000     2.000000  \n",
      "50%       6.000000     4.000000  \n",
      "75%       8.000000     7.000000  \n",
      "max      10.000000    10.000000  \n"
     ]
    }
   ],
   "source": [
    "print(features.iloc[:, 37:54].describe()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scale was also used for features **13 to 21 (always exclusive)** (`attr_o` to `prob_o`), **82 to 90** (`attr` to `prob`), **92 to 98** (`attr1_s` to `shar1_s`) and **98 to 104** (`attr3_s` to `shar3_s`).\n",
    "\n",
    "To address these scale violations, we can use the `.clip()` method to ensure that any value above 10 is set to 10 and any value below 1 is set to 1, thereby keeping all values within the valid range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            sports     tvsports     exercise       dining      museums  \\\n",
      "count  8299.000000  8299.000000  8299.000000  8299.000000  8299.000000   \n",
      "mean      6.425232     4.575491     6.245813     7.783829     6.987950   \n",
      "std       2.619024     2.801874     2.418858     1.754868     2.045364   \n",
      "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "25%       4.000000     2.000000     5.000000     7.000000     6.000000   \n",
      "50%       7.000000     4.000000     6.000000     8.000000     7.000000   \n",
      "75%       9.000000     7.000000     8.000000     9.000000     9.000000   \n",
      "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
      "\n",
      "               art       hiking       gaming     clubbing      reading  \\\n",
      "count  8299.000000  8299.000000  8299.000000  8299.000000  8299.000000   \n",
      "mean      6.716713     5.739246     3.850705     5.748162     7.660080   \n",
      "std       2.257442     2.565783     2.491490     2.497665     1.971051   \n",
      "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "25%       5.000000     4.000000     2.000000     4.000000     7.000000   \n",
      "50%       7.000000     6.000000     3.000000     6.000000     8.000000   \n",
      "75%       8.000000     8.000000     6.000000     8.000000     9.000000   \n",
      "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
      "\n",
      "                tv      theater       movies     concerts        music  \\\n",
      "count  8299.000000  8299.000000  8299.000000  8299.000000  8299.000000   \n",
      "mean      5.304133     6.778287     7.921798     6.827570     7.851066   \n",
      "std       2.529135     2.229052     1.691437     2.149909     1.791827   \n",
      "min       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "25%       3.000000     5.000000     7.000000     5.000000     7.000000   \n",
      "50%       6.000000     7.000000     8.000000     7.000000     8.000000   \n",
      "75%       7.000000     9.000000     9.000000     8.000000     9.000000   \n",
      "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
      "\n",
      "          shopping         yoga  \n",
      "count  8299.000000  8299.000000  \n",
      "mean      5.631281     4.343535  \n",
      "std       2.608913     2.711472  \n",
      "min       1.000000     1.000000  \n",
      "25%       4.000000     2.000000  \n",
      "50%       6.000000     4.000000  \n",
      "75%       8.000000     7.000000  \n",
      "max      10.000000    10.000000  \n"
     ]
    }
   ],
   "source": [
    "# Make sure features with 1-10 Scale from all waves are correctly\n",
    "features.iloc[:, 13:21] = features.iloc[:, 13:21].clip(1, 10)  # attr_o to prob_o\n",
    "features.iloc[:, 28:29] = features.iloc[:, 28:29].clip(1, 10)  # imprace\n",
    "features.iloc[:, 37:54] = features.iloc[:, 37:54].clip(1, 10)  # sports to yoga\n",
    "features.iloc[:, 82:90] = features.iloc[:, 82:90].clip(1, 10)  # attr to prob\n",
    "features.iloc[:, 92:98] = features.iloc[:, 92:98].clip(1, 10)  # attr1_s to shar1_s\n",
    "features.iloc[:, 98:104] = features.iloc[:,98:106].clip(1, 10)  # attr3_s to shar3_s\n",
    "print(features.iloc[:, 37:54].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `met` and its counterpart for the opposite partner, `met_o`, represent the question: *\"Have you met this person before?\"* The responses are binary (yes or no); however, they were recorded in the dataset in an unconventional manner, with *yes = 1* and *no = 2*.\n",
    "\n",
    "We assume that any value greater than 2 indicates that the person was known, so we clip these values to 2. Similarly, for the opposite side, values smaller than 1 are clipped to the lower bound of the interval (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 'met' Feature Description:\n",
      "count    8003.000000\n",
      "mean        0.948769\n",
      "std         0.989889\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         2.000000\n",
      "max         8.000000\n",
      "Name: met, dtype: float64\n",
      "\n",
      "Initial 'met_o' Feature Description:\n",
      "count    7993.000000\n",
      "mean        1.960215\n",
      "std         0.245925\n",
      "min         1.000000\n",
      "25%         2.000000\n",
      "50%         2.000000\n",
      "75%         2.000000\n",
      "max         8.000000\n",
      "Name: met_o, dtype: float64\n",
      "\n",
      "' met' Feature Description After Clipping:\n",
      "count    8003.000000\n",
      "mean        1.450456\n",
      "std         0.497570\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max         2.000000\n",
      "Name: met, dtype: float64\n",
      "\n",
      "' met_o' Feature Description After Clipping:\n",
      "count    7993.000000\n",
      "mean        1.956212\n",
      "std         0.204637\n",
      "min         1.000000\n",
      "25%         2.000000\n",
      "50%         2.000000\n",
      "75%         2.000000\n",
      "max         2.000000\n",
      "Name: met_o, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print initial descriptions for 'met' and 'met_o' features\n",
    "print(\"Initial 'met' Feature Description:\")\n",
    "print(features['met'].describe())\n",
    "print(\"\\nInitial 'met_o' Feature Description:\")\n",
    "print(features['met_o'].describe())\n",
    "\n",
    "# Clip the 'met' and 'met_o' features to the interval [1, 2]\n",
    "features['met'] = features['met'].clip(lower=1, upper=2)\n",
    "features['met_o'] = features['met_o'].clip(lower=1, upper=2)\n",
    "\n",
    "# Print the descriptions after clipping\n",
    "print(\"\\n' met' Feature Description After Clipping:\")\n",
    "print(features['met'].describe())\n",
    "print(\"\\n' met_o' Feature Description After Clipping:\")\n",
    "print(features['met_o'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6.3. Normalize Numerical Features collected using the Contingent Scale\n",
    "\n",
    "Keep in mind that some groups of attributes were collected based on the following restriction: _\"You have 100 points to distribute among the following attributes – give more points to those attributes that you think your fellow men/women find more important in a potential date, and fewer points to those attributes that they find less important in a potential date. The total points must equal 100.\"_\n",
    "\n",
    "Since we have already preprocessed the attributes from waves 6–9 following the methodology used by the survey authors in **Point 5 of this notebook**, we expect the total score across all these attributes within the corresponding group of related features to sum to 100. At this point, we will check whether instances outside waves 6–9 violate this restriction.\n",
    "\n",
    "We use a threshold of 0.2 to account for rounding errors and ignore entries that are missing, i.e., those that sum to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with sum not equal to 100: 819\n"
     ]
    }
   ],
   "source": [
    "# Function to check and display the sums\n",
    "def check_sums(feature_chain):\n",
    "    feature_chain = feature_chain.apply(pd.to_numeric, errors='coerce') # Ensure all values are numeric\n",
    "    non_100_rows = []\n",
    "\n",
    "    # Iterate over each row and calculate the sum\n",
    "    for index, row in feature_chain.iterrows():\n",
    "        row_sum = row.sum()\n",
    "        # If the sum is not equal to 100 (within a tolerance range), and not 0\n",
    "        if (row_sum <= 99.8 or row_sum >= 100.2) and row_sum != 0:\n",
    "            non_100_rows.append((index, row_sum))  # Store the row with a sum not equal to 100\n",
    "\n",
    "    return non_100_rows\n",
    "\n",
    "# Collect results for the different column ranges\n",
    "non_100_results = []\n",
    "non_100_results.extend(check_sums(features.iloc[:, 54:60]))  # attr1_1,...\n",
    "non_100_results.extend(check_sums(features.iloc[:, 60:66]))  # attr4_1,...\n",
    "non_100_results.extend(check_sums(features.iloc[:, 66:72]))  # attr2_1,...\n",
    "non_100_results.extend(check_sums(features.iloc[:, 7:13]))  # pf_o_att to pf_o_sha\n",
    "\n",
    "# Display all rows with a sum not equal to 100\n",
    "print(f\"Rows with sum not equal to 100: {len(non_100_results)}\")\n",
    "# for row in non_100_results:\n",
    "#     print(f\"Row index: {row[0]}, Sum: {row[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the restriction is violated 819 times. We define a method called `scale_to_100`, which proportionally scales the rows so that their sum becomes 100. This method will then be applied to all features within the corresponding group of attributes, **ensuring that the distribution sums to 100 points for each category**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale features to 100 points in sum if done wrong\n",
    "def scale_to_100(feature_chain):\n",
    "    feature_chain = feature_chain.apply(pd.to_numeric, errors='coerce')  # Ensure all values in the selected columns are numeric\n",
    "\n",
    "    # Check rows that do not sum to 100\n",
    "    non_100_rows = check_sums(feature_chain)\n",
    "\n",
    "    # Iterate over each row and scale it proportionally to sum to 100\n",
    "    for index, row_sum in non_100_rows:\n",
    "        if row_sum != 0:  # Skip rows where sum is zero\n",
    "            row = feature_chain.loc[index]\n",
    "            scaling_factor = 100 / row_sum  # Calculate the scaling factor\n",
    "            feature_chain.loc[index] = row * scaling_factor  # Scale each value in the row\n",
    "\n",
    "    return feature_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cell_id": "571810839ebb4c6387d2a459680a28c0",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "2dff60c8-2c90-44fb-ab5c-e9d743c1fc04",
    "execution_millis": 1772,
    "execution_start": 1733669861386,
    "source_hash": "fd4fef4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with sum not equal to 100: 0\n",
      "   pf_o_att  pf_o_sin  pf_o_int  pf_o_fun  pf_o_amb  pf_o_sha\n",
      "0     17.39     17.39     15.22     17.39     13.04     19.57\n",
      "1     20.00     20.00     20.00     20.00      6.67     13.33\n",
      "2     18.75     16.67     18.75     20.83     12.50     12.50\n",
      "3     18.60     16.28     18.60     18.60     11.63     16.28\n",
      "4     20.83     20.83     16.67     16.67      6.25     18.75\n",
      "5     17.39     17.39     15.22     17.39     13.04     19.57\n",
      "6     20.00     20.00     20.00     20.00      6.67     13.33\n",
      "7     18.75     16.67     18.75     20.83     12.50     12.50\n",
      "8     18.60     16.28     18.60     18.60     11.63     16.28\n",
      "9     20.83     20.83     16.67     16.67      6.25     18.75\n"
     ]
    }
   ],
   "source": [
    "# Apply scaling to specific columns for each row in the DataFrame\n",
    "features.iloc[:, 54:60] = scale_to_100(features.iloc[:, 54:60])  # attr1_1,...\n",
    "features.iloc[:, 60:66] = scale_to_100(features.iloc[:, 60:66])  # attr4_1,...\n",
    "features.iloc[:, 66:72] = scale_to_100(features.iloc[:, 66:72])  # attr2_1,...\n",
    "features.iloc[:, 7:13] = scale_to_100(features.iloc[:, 7:13])  # pf_o_att to pf_o_sha\n",
    "\n",
    "# Now check if the sums for these columns are correct\n",
    "non_100_results = []\n",
    "non_100_results.extend(check_sums(features.iloc[:, 54:60]))  # attr1_1,...\n",
    "non_100_results.extend(check_sums(features.iloc[:, 60:66]))  # attr4_1,...\n",
    "non_100_results.extend(check_sums(features.iloc[:, 66:72]))  # attr2_1,...\n",
    "non_100_results.extend(check_sums(features.iloc[:, 7:13]))  # pf_o_att to pf_o_sha\n",
    "\n",
    "# Output the results to verify if there are any rows where the sum is still not 100\n",
    "print(f\"Rows with sum not equal to 100: {len(non_100_results)}\")\n",
    "print(features.iloc[:10, 7:13])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7. Feature Engineering\n",
    "\n",
    "It is noticeable that in each instance, features like `zipcode`, `from` or features of interests only appear for one participant, while the corresponding values for their partner are not recorded. This is somewhat problematic, as it only provides information about one of the two individuals.  \n",
    "\n",
    "Therefore, using the **partner ID** (`pid`), we will retrieve and add the missing values for features like `zipcode` and `from` as `o.zipcode` and `o.from`, respectively.\n",
    "\n",
    "First, we check whether `pid` functions as a **primary key** for `iid` and whether every `pid` has a corresponding partner in `iid`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 instances have 'pid' without a corresponding entry in 'iid'.\n",
      "Examples of missing 'pid' values: [nan]\n",
      "\n",
      "First 10 rows with missing 'pid' values:\n",
      "      iid  gender  pid  int_corr  samerace  age_o race_o  pf_o_att  pf_o_sin  \\\n",
      "3317  122       1  NaN     -0.12         0    NaN    NaN       NaN       NaN   \n",
      "3327  123       1  NaN     -0.29         0    NaN    NaN       NaN       NaN   \n",
      "3337  124       1  NaN     -0.05         0    NaN    NaN       NaN       NaN   \n",
      "3347  125       1  NaN      0.15         0    NaN    NaN       NaN       NaN   \n",
      "3357  126       1  NaN      0.01         0    NaN    NaN       NaN       NaN   \n",
      "3367  127       1  NaN      0.38         0    NaN    NaN       NaN       NaN   \n",
      "3377  128       1  NaN     -0.05         0    NaN    NaN       NaN       NaN   \n",
      "3387  129       1  NaN      0.09         0    NaN    NaN       NaN       NaN   \n",
      "3397  130       1  NaN     -0.40         0    NaN    NaN       NaN       NaN   \n",
      "3407  131       1  NaN     -0.14         0    NaN    NaN       NaN       NaN   \n",
      "\n",
      "      pf_o_int  ...  sinc1_s  intel1_s  fun1_s  amb1_s  shar1_s  attr3_s  \\\n",
      "3317       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3327       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3337       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3347       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3357       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3367       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3377       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3387       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3397       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "3407       NaN  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
      "\n",
      "      sinc3_s  intel3_s  fun3_s  amb3_s  \n",
      "3317      NaN       NaN     NaN     NaN  \n",
      "3327      NaN       NaN     NaN     NaN  \n",
      "3337      NaN       NaN     NaN     NaN  \n",
      "3347      NaN       NaN     NaN     NaN  \n",
      "3357      NaN       NaN     NaN     NaN  \n",
      "3367      NaN       NaN     NaN     NaN  \n",
      "3377      NaN       NaN     NaN     NaN  \n",
      "3387      NaN       NaN     NaN     NaN  \n",
      "3397      NaN       NaN     NaN     NaN  \n",
      "3407      NaN       NaN     NaN     NaN  \n",
      "\n",
      "[10 rows x 103 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if all values in 'pid' also exist in 'iid'\n",
    "missing_pids = features[~features['pid'].isin(features['iid'])]\n",
    "\n",
    "# Output the result\n",
    "if len(missing_pids) == 0:\n",
    "    print(\"Every value in 'pid' has a corresponding entry in 'iid'.\")\n",
    "else:\n",
    "    print(f\"{len(missing_pids)} instances have 'pid' without a corresponding entry in 'iid'.\")\n",
    "    print(\"Examples of missing 'pid' values:\", missing_pids['pid'].unique()[:10])  # Display the first 10 missing 'pid' values\n",
    "    \n",
    "    # Print the first 10 rows where 'pid' does not have a corresponding entry in 'iid'\n",
    "    print(\"\\nFirst 10 rows with missing 'pid' values:\")\n",
    "    print(missing_pids.head(10))  # Print the first 10 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the missing `pid` values in the `features` dataset are actually `NaN` entries, not valid `pid` values. Looking at these 10 instances, it is evident that they mostly consist of `NaN` values. This suggests that there was a disruption during the data collection and documentation process. At this point, we can safely drop these 10 instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping missing 'pid': 8378\n",
      "Number of rows after dropping missing 'pid': 8368\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows before dropping\n",
    "print(\"Number of rows before dropping missing 'pid':\", features.shape[0])\n",
    "\n",
    "# Drop the rows where 'pid' is NaN\n",
    "features = features.dropna(subset=['pid'])\n",
    "\n",
    "# Print the number of rows after dropping\n",
    "print(\"Number of rows after dropping missing 'pid':\", features.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we merge the respective attributes of the date partner to the corresponding instance.\n",
    "\n",
    "Finally, we drop `iid` (unique subject number, group/wave ID, gender) and `pid` (partner's `iid` number), as they are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the desired columns\n",
    "filtered_df = features[[\"iid\", \"zipcode\", \"from\", \"field_cd\", \"undergra\", \"mn_sat\", \"tuition\", \"imprace\", \n",
    "                        \"imprelig\", \"income\", \"goal\", \"date\", \"go_out\", \"career_c\", \"sports\", \"tvsports\",\n",
    "                        \"exercise\", \"dining\", \"museums\", \"art\", \"hiking\", \"gaming\", \"clubbing\", \"reading\", \n",
    "                        \"tv\", \"theater\", \"movies\", \"concerts\", \"music\", \"shopping\", \"yoga\"]].drop_duplicates()\n",
    "\n",
    "# Merge 'features' and 'filtered_df' based on 'pid' and 'iid'\n",
    "merged_df = features.merge(filtered_df, left_on='pid', right_on='iid', suffixes=('', '_partner'))\n",
    "\n",
    "# Assign all relevant partner attributes to 'features'\n",
    "for col in filtered_df.columns:\n",
    "    if col != \"iid\":  # Avoid copying 'iid' itself\n",
    "        features[f\"o.{col}\"] = merged_df[f\"{col}_partner\"]\n",
    "\n",
    "# Drop 'pid' and 'iid' columns from 'features'\n",
    "features = features.drop(columns=['pid', 'iid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we aim to generate additional binary features based on similarities and differences in certain feature values. The authors had already done this for the feature `samerace`. We will extend this approach to `sameZipcode`, `sameFrom`, `sameField`, `sameUndergra`, `sameGoal`, and `sameCareer`. To achieve this, we will examine the values of the respective features and compare them with their counterparts/opposites to check if the entries match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sameZipcode  sameFrom  sameField  sameUndergra  sameGoal  sameCareer\n",
      "0          0.0       0.0        1.0           0.0       1.0         0.0\n",
      "1          0.0       0.0        1.0           0.0       1.0         1.0\n",
      "2          0.0       0.0        0.0           0.0       1.0         0.0\n",
      "3          0.0       0.0        1.0           0.0       1.0         1.0\n",
      "4          0.0       0.0        1.0           0.0       1.0         1.0\n"
     ]
    }
   ],
   "source": [
    "# Iterating through all instances and comparing the values of the respective features\n",
    "for index, row in features.iterrows():\n",
    "    # Compare the values of each feature with its counterpart\n",
    "    features.at[index, 'sameZipcode'] = 1 if row['zipcode'] == features.at[index, 'o.zipcode'] else 0\n",
    "    features.at[index, 'sameFrom'] = 1 if row['from'] == features.at[index, 'o.from'] else 0\n",
    "    features.at[index, 'sameField'] = 1 if row['field_cd'] == features.at[index, 'o.field_cd'] else 0\n",
    "    features.at[index, 'sameUndergra'] = 1 if row['undergra'] == features.at[index, 'o.undergra'] else 0\n",
    "    features.at[index, 'sameGoal'] = 1 if row['goal'] == features.at[index, 'o.goal'] else 0\n",
    "    features.at[index, 'sameCareer'] = 1 if row['career_c'] == features.at[index, 'o.career_c'] else 0\n",
    "\n",
    "# Print a few rows to verify the newly created features\n",
    "print(features[['sameZipcode', 'sameFrom', 'sameField', 'sameUndergra', 'sameGoal', 'sameCareer']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a1450e6e04314a3d9d29ee34e153b69b",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "markdown",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8.  Save Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "cell_id": "a9a4bf5f223f44f199bf4fd21b73088d",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "2dff60c8-2c90-44fb-ab5c-e9d743c1fc04",
    "execution_millis": 874,
    "execution_start": 1733669905754,
    "source_hash": "35a97941"
   },
   "outputs": [],
   "source": [
    "# Convert numpy arrays to 1D and then to pandas Series\n",
    "target_match = pd.Series(target_match, name='target_match')\n",
    "\n",
    "# Now save them as CSV files\n",
    "features.to_csv('../../data/processed/features.csv', index=False)\n",
    "target_match.to_csv('../../data/processed/target_match.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_notebook_id": "a0b03567b83a44f5b71bed54d605ac87",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
